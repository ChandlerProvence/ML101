{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61208b6d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#  <span style=\"color:#0b186c;\">Introduction to DL: Perceptrons</span>\n",
    "\n",
    "---\n",
    "\n",
    "“Deep learning is a subset of machine learning that's based on artificial neural networks. The learning process is deep because the structure of artificial neural networks consists of multiple input, output, and hidden layers. Each layer contains units that transform the input data into information that the next layer can use for a certain predictive task. Thanks to this structure, a machine can learn through its own data processing.” - Microsoft, 2022\n",
    "\n",
    "\n",
    "\n",
    "<br></br>\n",
    "\n",
    "## <span style=\"color:#0b186c;\">Table of Contents:</span>\n",
    "* [History of Perceptrons](#first-bullet)\n",
    "* [Dataset Information](#second-bullet)\n",
    "* [Multilayer Perceptron (MLP)](#third-bullet)\n",
    "* [Conclusion](#fourth-bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ce6260",
   "metadata": {},
   "source": [
    "#  <span style=\"color:#0b186c;\">History of Perceptrons</span><a class=\"anchor\" id=\"first-bullet\"></a>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "Perceptron learning is a fundamental concept in neural networks and forms the basis for more complex artificial neural networks used in deep learning. The mathematical modeling of a perceptron dates back as far as the 1940s with the first implementation being introduced in the late 1950s by an American psychologist, Frank Rosenblatt. The original intent of the perceptron was to create a physical machine capable of mimicing the learning behavior and process of a human neuron. The perceptron consisted of one or more numerical input features, weights associated with each input, and a bias that was added to the weighted sum of the inputs. These components would produce an output similar to the dependent y output discussed in traditional machine learning. However, the original perceptron was only able to perform well on data points that could be linearly separated. \n",
    "\n",
    "Advances from the original single-layered perceptron included the expansion into multiple layers and the addition of activation functions to apply non-linear transformations to the neuron outputs at each layer. These advances have culminated over the last few decades to bring us to where we are now in the world of Deep Learning with `Multi-layer Perceptrons` (MLP) and `Artificial Neural Networks` (ANNs). These foundational algorithms are at the core of what has driven the rapid expanse and development of cutting edge technologies, like our work on `Arcas`, since the late 1990s. \n",
    "\n",
    "\n",
    "## <span style=\"color:#0b186c;\">Required Imports:</span>\n",
    "\n",
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "<b>Note:</b> If you have not previously installed these `packages`, you can use the cell below to perform the required `pip` installs.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5992e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you still need to perform some pip installs:\n",
    "! pip install --user pandas -q\n",
    "! pip install --user numpy -q\n",
    "! pip install --user scikit-learn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e1fc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe and array libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Libraries for visualizing data\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Retrieves the dataset from Scikit-learn\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Required for performing standardization\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Required for training and validating a model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Required for instantiating and running the MLP neural network\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Classification metrics and confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "\n",
    "# Filters out warning messages\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b908345b",
   "metadata": {},
   "source": [
    "#  <span style=\"color:#0b186c;\">Dataset Information</span><a class=\"anchor\" id=\"second-bullet\"></a>\n",
    "\n",
    "---\n",
    "\n",
    "We will be using a dataset containing 3 species in the Iris genus, namely, Iris Setosa, Iris Versicolor and Iris Virginica found in the Gaspé Peninsula. For the purposes of an integral study, the collected Iris samples were, \"all from the same pasture, and picked on the same day and measured at the same time by the same person with the same apparatus.\" The dataset contains 150 rows of data, 50 rows of data for each species of Iris flower. The column names represent the feature of the flower that was studied and recorded.\n",
    "\n",
    "Our target dataset can be found in the Scikit-learn library, so we will be importing it directly from the library and storing it into a Pandas dataframe.\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837777dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the iris dataset\n",
    "iris = load_iris(as_frame=True)\n",
    "\n",
    "# Place the dataset into a dataframe\n",
    "df = iris.frame \n",
    "\n",
    "# View the first 5 records in the dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b32391",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "A sepal is a part of the flower of angiosperms. Usually green, sepals typically function as protection for the flower in bud, and often as support for the petals when in bloom. Petals are modified leaves that surround the reproductive parts of flowers. They are often brightly colored or unusually shaped to attract pollinators. \n",
    "\n",
    "\n",
    "\n",
    "The petal and sepal measurement values allow us to look for patterns related to the specific species of Iris flower.\n",
    "\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "   \n",
    "We can use the `.info()` method for our dataframe to view a concise summary of the information contained within. This includes the number of observations, columns and data types, and any missing values.\n",
    "    \n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b63a406",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0513e074",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "   \n",
    "For numerical features in the dataframe, we can use the `.describe()` method to view relevant statistical information about each of the features. Understanding these values can assist in identifying the presence of outliers.\n",
    "    \n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a24ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d19616",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "   \n",
    "Additionally, we can use a `.pairplot()` from the `seaborn` library to visualize a scatter matrix of the independent variables. We can color code the plotted points based on the `target` feature to identify any discernable patterns in the measurement values.\n",
    "    \n",
    " </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cb4f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "sns.set(rc={'figure.figsize':(12,8),'ytick.labelsize':(12)})\n",
    "\n",
    "# Create a pairplot\n",
    "sns.pairplot(df, hue = \"target\", palette = \"Set2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c5d49c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "   \n",
    "Our dataset contains an equal number of observations for each of the Iris flowers. We can visualize the target variable distributions with a pie chart:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1139a1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pie chart for the target variable\n",
    "df.target.value_counts().plot(kind='pie', figsize=(8, 8), fontsize=10, autopct='%1.0f%%')\n",
    "plt.title(\"Target Variable Distribution\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0d8ac0",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "   \n",
    "Lastly, we can use the `.corr()` method on our dataframe to identify linear relationships between the independent variables and the dependent variable. This also helps identify collinearity that may exist amongst the independent variables as well. The correlation matrix can be enhanced by using a `.heatmap()` from the `seaborn` library that scales the specified hue based on the severity of the linear relationship.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf722ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "sns.set(rc={'figure.figsize':(12,8),'ytick.labelsize':(12)})\n",
    "\n",
    "# Use the corr method to create the correlation matrix\n",
    "correlation_matrix = df.corr().round(2)\n",
    "\n",
    "# Create a heatmap based on the severity of the linear relationship\n",
    "sns.heatmap(data = correlation_matrix, annot = True, cmap = \"Blues\")\n",
    "plt.title(\"Variable Correlation Heatmap\\n\", fontsize = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de651e0",
   "metadata": {},
   "source": [
    "#  <span style=\"color:#0b186c;\">Multilayer Perceptron (MLP)</span><a class=\"anchor\" id=\"third-bullet\"></a>\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "A Mulilayer Perceptron (MLP) network is a fully connected, feedforward ANN consisting of an input layer, one or more hidden layers, and an output layer. This type of network is sometimes used to ambiguously refer to *any* type of ANN, however, it can also refer to specific ones (e.g., specific activation functions, specific perceptron algorithm variations, etc.). \n",
    "\n",
    "The `scikit-learn` library includes 2 variations of an MLP model in the form of a regressor and a classifier. The primary difference between them is the loss and activation functions. Since the iris dataset includes a discrete, categorical target variable, we will be using the `MLPClassifier()`.\n",
    "\n",
    "As seen on the documentation page, the MLP model has a predefined `loss` and `optimizer` function to determine how it learns. Since the model doesn't know the relationship between the x and y values, it has to guess. The loss function defines how the effectiveness of the model's guess is evaluated and scored by comparing the guessed answers with the known correct answers in the dataset. After each guess, the optimizer defines the logic used to update the weights learned by the model to minimize the loss function.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "\n",
    "<div class=\"alert alert-success\">\n",
    "    \n",
    "First, split the data into independent variables (X) and the dependent target variable (y):\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6664fbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the independent (X) and dependent (y) variables\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# Review the input variables without a target label\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a99cfb",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "&nbsp;**Note:** It is imperative that the subsets are representative of the whole &nbsp;dataset. \n",
    "The best way to accomplish this is using the built-in function, &nbsp;`train_test_split()`.\n",
    "\n",
    "</div>\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efd50aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the independent (X) and dependent (y) variables\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1].values\n",
    "\n",
    "# Split the data into an 80/20 \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n",
    "\n",
    "# Output the shape of the training set\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caebb78",
   "metadata": {},
   "source": [
    "Using **standardization**, we can change the form of our features into a normal distribution, so that it easier to correctly represent the feature weights in the modeling process.\n",
    "\n",
    "The `StandardScaler()` from `scikit-learn` standardizes independently on each feature by setting the mean to 0 and the standard deviation to 1 to accomplish the scaling appropriately. First, the scaler has to be fit on the training data to learn the relevant statistics. Using the `.fit_transform()` method, we can fit and simultaneously transform the training data in a single line of code. The test data is then transformed using the `.transform()` method.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf746fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the standard scaler\n",
    "sc = StandardScaler()\n",
    "\n",
    "# Fit and transform the scaler on the training set\n",
    "X_train = sc.fit_transform(X_train) \n",
    "\n",
    "# Transform the fit scaler on the test set\n",
    "X_test = sc.transform(X_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c5be7f",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    \n",
    "&nbsp;**Note:** The output layer activation function is determined automatically by the `MLPClassifier` internally. Binary classification problems will use the `Logistic` activation function and multi-class classification problems will use the `Softmax` activation function.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccd7743",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the MLP ANN\n",
    "MLP = MLPClassifier()\n",
    "\n",
    "# Fit the ANN on the training data\n",
    "MLP.fit(X_train, y_train)\n",
    "\n",
    "# Verify the output activation function is softmax\n",
    "MLP.out_activation_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7f706d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions based on the X values in the test set\n",
    "y_pred = MLP.predict(X_test)\n",
    "\n",
    "# Calculate the accuracy score of the test set\n",
    "score = round((accuracy_score(y_test, y_pred) * 100), 2)\n",
    "\n",
    "#Plot the confusion Matrix for the predictions\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm_display = ConfusionMatrixDisplay(cm)\n",
    "cm_display = cm_display.plot(include_values=True, cmap='Blues', ax=None, xticks_rotation='horizontal')\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "# Print the accuracy score on the validation data\n",
    "print(f\"Accuracy = {score}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c4cd07",
   "metadata": {},
   "source": [
    "#  <span style=\"color:#0b186c;\">Conclusion</span><a class=\"anchor\" id=\"fourth-bullet\"></a>\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
